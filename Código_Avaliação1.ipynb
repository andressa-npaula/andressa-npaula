{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1i1Dl3jmak5zUBMmc2jTh9jsUWCO4tdlD","timestamp":1749570824431}],"authorship_tag":"ABX9TyN1iJ3w/8/JoYTeh8DB8DOb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# AVALIAÇÃO 1 - PROGRAMAÇÃO DINÂMICA ESTOCÁSTICA\n","import numpy as np\n","\n","# Estados e ações do ambiente\n","estados = ['s1', 's2']\n","acoes = {'s1': ['x11', 'x12'], 's2': ['x21', 'x22']}\n","\n","# Recompensa para cada estado e ação\n","def recompensa(estado, acao):\n","    if estado == 's1':\n","        return 1\n","    else:\n","        return 0\n","\n","# Probabilidades de transição (estado, ação -> próximo estado)\n","prob_transicao = {\n","    ('s1', 'x11'): {'s1': 0.5, 's2': 0.5},\n","    ('s1', 'x12'): {'s1': 0.7, 's2': 0.3},\n","    ('s2', 'x21'): {'s1': 0.1, 's2': 0.9},\n","    ('s2', 'x22'): {'s1': 0.9, 's2': 0.1}\n","}\n","\n","# Fator de desconto e limite de convergência\n","fator_desconto = 0.9\n","limite_convergencia = 0.01\n","\n","# (a) Iteração de Valor\n","def iteracao_valor():\n","    valores = {estado: 0.0 for estado in estados}\n","    while True:\n","        delta = 0\n","        valores_novos = valores.copy()\n","        for estado in estados:\n","            melhores_valores_acao = []\n","            for acao in acoes[estado]:\n","                valor_acao = recompensa(estado, acao) + fator_desconto * sum(\n","                    prob_transicao[(estado, acao)][prox_estado] * valores[prox_estado]\n","                    for prox_estado in estados\n","                )\n","                melhores_valores_acao.append(valor_acao)\n","            valores_novos[estado] = max(melhores_valores_acao)\n","            delta = max(delta, abs(valores_novos[estado] - valores[estado]))\n","        valores = valores_novos\n","        if delta < limite_convergencia:\n","            break\n","    return valores\n","\n","# (b) Política ótima a partir dos valores\n","def obter_politica_gulosa(valores):\n","    politica = {}\n","    for estado in estados:\n","        melhor_acao = None\n","        melhor_valor = -float('inf')\n","        for acao in acoes[estado]:\n","            valor_acao = recompensa(estado, acao) + fator_desconto * sum(\n","                prob_transicao[(estado, acao)][prox_estado] * valores[prox_estado]\n","                for prox_estado in estados\n","            )\n","            if valor_acao > melhor_valor:\n","                melhor_valor = valor_acao\n","                melhor_acao = acao\n","        politica[estado] = melhor_acao\n","    return politica\n","\n","# (c) Iteração de Política\n","def iteracao_politica():\n","    politica = {estado: acoes[estado][0] for estado in estados}\n","    valores = {estado: 0.0 for estado in estados}\n","    estavel = False\n","    while not estavel:\n","        # Avaliação\n","        while True:\n","            delta = 0\n","            valores_novos = valores.copy()\n","            for estado in estados:\n","                acao = politica[estado]\n","                valores_novos[estado] = recompensa(estado, acao) + fator_desconto * sum(\n","                    prob_transicao[(estado, acao)][prox_estado] * valores[prox_estado]\n","                    for prox_estado in estados\n","                )\n","                delta = max(delta, abs(valores_novos[estado] - valores[estado]))\n","            valores = valores_novos\n","            if delta < limite_convergencia:\n","                break\n","        # Melhoria\n","        estavel = True\n","        for estado in estados:\n","            acao_antiga = politica[estado]\n","            melhor_acao = acao_antiga\n","            melhor_valor = -float('inf')\n","            for acao in acoes[estado]:\n","                valor_acao = recompensa(estado, acao) + fator_desconto * sum(\n","                    prob_transicao[(estado, acao)][prox_estado] * valores[prox_estado]\n","                    for prox_estado in estados\n","                )\n","                if valor_acao > melhor_valor:\n","                    melhor_valor = valor_acao\n","                    melhor_acao = acao\n","            politica[estado] = melhor_acao\n","            if acao_antiga != melhor_acao:\n","                estavel = False\n","    return politica, valores\n","\n","# Execução e resultados\n","valores_otimos = iteracao_valor()\n","politica_otima_vi = obter_politica_gulosa(valores_otimos)\n","politica_otima_pi, valores_pi = iteracao_politica()\n","\n","print(\"Valor ótimo (Iteração de Valor):\", valores_otimos)\n","print(\"Política ótima (Iteração de Valor):\", politica_otima_vi)\n","print(\"Política ótima (Iteração de Política):\", politica_otima_pi)\n","print(\"Valores (Iteração de Política):\", valores_pi)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Q-9wJSUiFYG","executionInfo":{"status":"ok","timestamp":1749584194911,"user_tz":180,"elapsed":65,"user":{"displayName":"Paula Andressa Nascimento Lucas","userId":"05308594251867866960"}},"outputId":"27d2a94b-e613-4790-9957-9eb49b5697a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Valor ótimo (Iteração de Valor): {'s1': 7.622070542910445, 's2': 6.774612915791802}\n","Política ótima (Iteração de Valor): {'s1': 'x12', 's2': 'x22'}\n","Política ótima (Iteração de Política): {'s1': 'x12', 's2': 'x22'}\n","Valores (Iteração de Política): {'s1': 7.629598945767491, 's2': 6.782141318648846}\n"]}]}]}